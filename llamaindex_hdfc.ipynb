{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02511d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTSimpleVectorIndex, Document, SimpleDirectoryReader\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f87a234",
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43mSimpleDirectoryReader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mD:/billionlives/KuwaitBanks/BRSR\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\llama_index\\readers\\file\\base.py:153\u001b[0m, in \u001b[0;36mSimpleDirectoryReader.load_data\u001b[1;34m(self, concatenate)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mparser_config_set:\n\u001b[0;32m    152\u001b[0m         parser\u001b[38;5;241m.\u001b[39minit_parser()\n\u001b[1;32m--> 153\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# do standard read\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(input_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\llama_index\\readers\\file\\tabular_parser.py:103\u001b[0m, in \u001b[0;36mPandasCSVParser.parse_file\u001b[1;34m(self, file, errors)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_file\u001b[39m(\u001b[38;5;28mself\u001b[39m, file: Path, errors: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]]:\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;124;03m\"\"\"Parse file.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pandas_config)\n\u001b[0;32m    105\u001b[0m     text_list \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m row: (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_col_joiner)\u001b[38;5;241m.\u001b[39mjoin(row\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    107\u001b[0m     )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concat_rows:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1235\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:75\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     72\u001b[0m     kwds\u001b[38;5;241m.\u001b[39mpop(key, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     74\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ensure_dtype_objs(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m---> 75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:551\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader('D:/billionlives/KuwaitBanks/BRSR').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efeab49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 26676 tokens\n"
     ]
    }
   ],
   "source": [
    "index = GPTSimpleVectorIndex(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe58e627",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3761 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 17 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Not applicable. The entity does not have any air emissions other than GHG emissions, as indicated by the summary of Scope 1 and Scope 2 emissions provided.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\" Please provide details of air emissions (other than GHG emissions) by the entity ?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "624b1145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3806 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 8 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The total Scope 1 emission is 966.1 metric tonnes of CO2 equivalent, as reported in the Corporation's Social Impact Assessments (SIA) for the current financial year.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\" What is the total Scope 1 emission ?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b22c4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3842 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 8 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The total Scope 2 emission is 2,684.1 metric tonnes of CO2 equivalent, as reported in the Corporation's Social Impact Assessments (SIA) and Rehabilitation and Resettlement (R&R) projects for the current financial year. This figure represents the percentage of input material (inputs to total) used in the Corporation's operations.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\" What is the total Scope 2 emission ?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "777cebd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 4000 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 10 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The Corporation's usage of water is restricted to human consumption purposes only. Efforts have been made to ensure that water is consumed judiciously in the office premises. In various offices, sensor taps are installed in office washrooms to economise on water consumption. The Corporation ensures that the domestic waste (sewage) from offices and branches are not let into water bodies. Additionally, the Corporation has taken steps to reduce its carbon footprint by reducing emissions from generator sets, company cars, and fuel allowance. Metric tonnes of CO2 equivalent emissions have been reduced from 966.1 to 1,340.1 for Scope 2 emissions, and from 2,684.1 to 2,864.6 for Scope 1 emissions.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\" Provide details of the following disclosures related to water?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ea1c89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 4182 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 10 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Corporation has systems in place at an all-India level to manage e-waste. The Corporation engages with certified e-waste handlers for disposal of e-waste. Approximately 7,016 kgs of e-waste generated was disposed through authorised recyclers in FY21. The Corporation received green, disposable and re-cycling certificates from the respective e-waste vendors. The Corporation uses local vendors for disposal of paper for recycling. For several years, the sale value of paper scrap of the Corporation is being donated to Cancer Patients Aid Association. In an endeavour to reduce consumption of plastics, the Corporation, as a policy does not procure plastic bottled water in its offices as such plastics are not biodegradable and micro plastics release toxic chemicals into the environment. Additionally, the Corporation has implemented a performance management system to ensure that all employees are held accountable for their waste management practices. The system is reviewed by the board and the company secretary on an annual basis and is designed to review the previous year’s performance with specific reference to achievement of targets and give constructive feedback on performance, provide an opportunity for communication and interaction between the appraiser and appraisee regarding the previous year’s performance and setting of performance targets for the next appraisal\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\" Provide details related to waste management by the entity?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6ff0ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3790 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 12 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.214 metric tonnes of plastic waste was generated in FY21, out of the total input material used by the entity.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\" How much amount of plastic waste was generated in FY21?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bba16c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3712 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 13 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Approximately 7,016 kgs of e-waste generated was disposed through authorised recyclers in FY21, as part of the company's performance management system which seeks to ensure a fair and transparent system of appraisal and reward.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\" How much amount of E-waste recycled in FY21?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f521913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 4132 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 30 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The Corporation does not have any operations/offices in/around ecologically sensitive areas. However, the Corporation is committed to protecting the environment and has taken various initiatives to reduce its carbon footprint. These initiatives include the installation of solar panels in certain offices, the use of LED lighting, the installation of sensor taps in office washrooms to economise on water consumption, and the use of energy efficient equipment. The Corporation also ensures that the domestic waste (sewage) from offices and branches are not let into water bodies. Additionally, the Corporation has taken steps to reduce its Scope 1 and Scope 2 emissions, including the use of generator sets, company cars and petrol cards, and fuel allowance. The Corporation has also taken steps to reduce its GHG emissions, including CO2, CH4, N2O, HFCs, PFCs, SF6, and NF3.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"If the entity has operations/offices in/around ecologically sensitive areas  where environmental approvals / clearances are required, please specify details ? \")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbcbac3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 2877 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 22 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Not applicable.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"Details of environmental impact assessments of projects undertaken by the entity based on applicable laws, in the current financial year?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02a82172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 4149 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 29 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Yes, the entity is compliant with the applicable environmental law/ regulations/ guidelines in India. The Corporation has taken various measures to ensure that it is compliant with the applicable environmental law/ regulations/ guidelines in India. These measures include the installation of sensor taps in office washrooms to economise on water consumption, the implementation of a mechanism for Zero Liquid Discharge, and the monitoring of GHG emissions across the two financial years. The Corporation also ensures that the domestic waste (sewage) from offices and branches are not let into water bodies. Additionally, the Corporation has monitored and reported its Scope 1 and Scope 2 emissions, including CO2, CH4, N2O, HFCs, PFCs, SF6, and NF3, for the two financial years. The Corporation has also taken steps to reduce its emissions, such as the use of generator sets and company cars powered by petrol, and the provision of fuel allowance for petrol.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"Is the entity compliant with the applicable environmental law/ regulations/ guidelines in India.If not, provide details of all such non-compliances?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6232226f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 2885 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 31 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Not applicable.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"Provide the details of Social Impact Assessments (SIA) of projects undertaken by the entity based on applicable laws, in the current financial year?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "571b77d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 2883 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 29 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Not applicable.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"Provide information on project(s) for which ongoing Rehabilitation and Resettlement (R&R) is being undertaken by your entity. ?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7e96798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 4254 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 39 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "FY21 FY20\n",
      "Filed during the yearPending resolution at the end of the yearRemarksFiled during the yearPending resolution at the end of the yearRemarks\n",
      "Sexual Harassment2 1The pending case was received in the month of March 2021 and hence as at March 31, 2021, the internal complaints committee was in the process of examining the same.2 Nil\n",
      "Discrimination at workplaceNil Nil - Nil Nil -\n",
      "Child LabourNil Nil - Nil Nil -\n",
      "Forced Labour/Involuntary LabourNil Nil - Nil Nil -\n",
      "WagesNil Nil - Nil Nil -\n",
      "Other human rights related issuesNil Nil - Nil Nil -\n",
      "CO2 Emissions (MT)966.1 1,340.1\n",
      "CH4 Emissions (MT)<0.1 <0.1\n",
      "N2O Emissions (MT)<0.1 <0.1\n",
      "CO2e Emissions (MT)2,684.1 2,864.6\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"Details of complaints made by employees in the areas Sexual Harassement,Discrimination at workplace,Child Labour,Forced Labour/Involuntary Labour,Wages and Other human right issues ?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "654b5922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3041 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 8 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The assessments for the year include Social Impact Assessments (SIA) of projects undertaken by the entity, Rehabilitation and Resettlement (R&R) of projects, mechanisms to receive and redress grievances of the community, input material sourced from suppliers, corrective actions taken to mitigate any negative social impacts identified in the Social Impact Assessments, benefits derived and shared from the intellectual properties owned or acquired by the entity, instances of product recalls on account of safety issues, cyber security and risks related to data privacy, consumer complaints in respect of data privacy, advertising, cyber-security, restrictive trade practices, unfair trade practices, and other issues, corrective actions taken or underway on issues relating to advertising and delivery of essential services, cyber security and data privacy of customers, re-occurrence of instances of product recalls, penalty/action taken by regulatory authorities on safety of products/services, and data breaches.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"What are the assessments for the year?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a11329e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 4333 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 11 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The underlying philosophy of the performance management system is to have a fair and transparent system of appraisal, which ensures an objective mechanism to measure each employee’s performance and potential and implement a reward system which recognises merit. The performance appraisal system has been designed to achieve the following objectives:\n",
      "• Review the previous year’s performance with specific reference to achievement of targets and give constructive feedback on performance;\n",
      "• Provide an opportunity for communication and interaction between the appraiser and appraisee regarding the previous year’s performance and setting of performance targets for the next appraisal period; and\n",
      "• Reward employees who have performed well during the appraisal period and those who demonstrate the ability to handle higher responsibilities with promotions/increased job responsibilities.\n",
      "\n",
      "The Corporation has a policy on health and safety for its employees, its group companies, communities and NGOs. The Corporation engages with stakeholders to identify and manage environmental and social topics, and incorporates the inputs received from stakeholders into policies and activities of the entity. Periodic internal communication and alerts are sent out to employees and awareness sessions are conducted on safety related aspects. Employees on a pan-India basis are given periodic training on basic and advanced fire safety, including evacuation drills. HDFC has tie-ups with vendors to educate and\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"Please provide the details of Health and safety management system \")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77d248aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 2914 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 59 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Not applicable.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"Please provide the details of Lost Time Injury Frequency Rate (LTIFR) (per one million-person hours worked),Total recordable work-related injuries,No. of fatalities (safety incident),High consequence work-related injury or ill-health (excluding fatalities) in FY21? \")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4ccf411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3886 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 17 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No complaints related to working conditions and health and safety were reported in FY21, excluding loan moratorium requests and Credit Linked Subsidy Scheme related issues.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"How many complaints related to working conditions and health and safety were reported in FY21?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c38391f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3627 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 18 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "It is not possible to answer this question with the given context information.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"How many male employees were trained On health and safety/wellness measures in FY21?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a12ef28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 4466 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 8 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The Corporation is engaged in providing housing finance solutions to individuals and corporate entities. The Corporation also provides a range of other financial services such as loans against property, loan against securities, working capital finance, loan against rent receivables, etc. The Corporation also provides a range of non-financial services such as advisory services, project finance, etc. The Corporation also has a presence in the capital markets through its subsidiaries and joint ventures. The Corporation also has a presence in the mutual fund industry through its joint venture with Standard Life Investments. The Corporation also has a presence in the life insurance industry through its joint venture with HDFC Standard Life Insurance Company Limited. The Corporation also has a presence in the general insurance industry through its joint venture with HDFC ERGO General Insurance Company Limited. The Corporation also has a presence in the asset management industry through its joint venture with HDFC Asset Management Company Limited. The Corporation also has a presence in the real estate industry through its subsidiary, HDFC Realty Limited. The Corporation also has a presence in the venture capital industry through its subsidiary, HDFC Ventures Limited. The Corporation also has a presence in the education finance industry through its subsidiary, HDFC Credila Financial Services Private Limited. The Corporation also has a presence in the microf\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"Provide the details of business activities?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c43bb1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3742 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 8 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are two board of directors (Whole-time directors).\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"How many board of directors are there?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "200714ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabula\n",
    "\n",
    "# Set the input and output file paths\n",
    "input_file = \"D:/billionlives/KuwaitBanks/BRSR/HDFC.pdf\"\n",
    "output_folder = \"D:/billionlives/KuwaitBanks/BRSR\"\n",
    "\n",
    "# Extract tables from the input file\n",
    "tables = tabula.read_pdf(input_file, pages=\"all\", multiple_tables=True)\n",
    "\n",
    "# Iterate through each table and save it as a CSV file\n",
    "for i, table in enumerate(tables):\n",
    "    output_file = f\"{output_folder}table_{i+1}.csv\"\n",
    "    table.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8766b7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabula-py in c:\\users\\joe\\anaconda3\\lib\\site-packages (1.4.3)\n",
      "Requirement already satisfied: distro in c:\\users\\joe\\anaconda3\\lib\\site-packages (from tabula-py) (1.8.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\joe\\anaconda3\\lib\\site-packages (from tabula-py) (1.21.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\joe\\anaconda3\\lib\\site-packages (from tabula-py) (1.4.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\joe\\anaconda3\\lib\\site-packages (from pandas->tabula-py) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\joe\\anaconda3\\lib\\site-packages (from pandas->tabula-py) (2.8.2)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\joe\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\joe\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\joe\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\joe\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\joe\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\joe\\anaconda3\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\joe\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->tabula-py) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pip install tabula-py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ad9bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import camelot\n",
    "import pandas as pd\n",
    "\n",
    "def extract_pdf_tables(pdf_file):\n",
    "    tables = camelot.read_pdf(pdf_file, pages='all', flavor='stream')\n",
    "    data = []\n",
    "    for table in tables:\n",
    "        data.append(table.df)\n",
    "    return data\n",
    "\n",
    "def filter_esg_data(tables):\n",
    "    esg_terms = ['Scope', 'Water Consumption', 'Electricity consumption']\n",
    "    esg_data = pd.DataFrame()\n",
    "    for table in tables:\n",
    "        for term in esg_terms:\n",
    "            if term in table.to_string():\n",
    "                esg_data = pd.concat([esg_data, table], ignore_index=True)\n",
    "                break\n",
    "    return esg_data\n",
    "\n",
    "def save_esg_data_to_csv(esg_data, csv_file):\n",
    "    esg_data.to_csv(csv_file + '.csv', index=False)\n",
    "\n",
    "pdf_file = 'D:/billionlives/KuwaitBanks/NBK.pdf'\n",
    "tables = extract_pdf_tables(pdf_file)\n",
    "esg_data = filter_esg_data(tables)\n",
    "csv_file = 'D:/billionlives/KuwaitBanks/NBKesg_data'\n",
    "save_esg_data_to_csv(esg_data, csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee8dd45c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Page' object has no attribute 'extract_tables'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Extract tables from each PDF page\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m pdf_file:\n\u001b[1;32m---> 24\u001b[0m         \u001b[43mextract_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_writer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Close CSV file\u001b[39;00m\n\u001b[0;32m     27\u001b[0m csv_file\u001b[38;5;241m.\u001b[39mclose()\n",
      "Input \u001b[1;32mIn [37]\u001b[0m, in \u001b[0;36mextract_tables\u001b[1;34m(page, csv_writer)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_tables\u001b[39m(page, csv_writer):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Get list of page tables as dicts\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     tables \u001b[38;5;241m=\u001b[39m \u001b[43mpage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mextract_tables\u001b[49m()\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Iterate over each table\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables:\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;66;03m# Get table rows as list of lists\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Page' object has no attribute 'extract_tables'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import fitz\n",
    "\n",
    "# Open PDF file\n",
    "pdf_file = fitz.open(\"D:/billionlives/KuwaitBanks/NBK.pdf\")\n",
    "\n",
    "# Define function to extract tables from a PDF page\n",
    "def extract_tables(page, csv_writer):\n",
    "    # Get list of page tables as dicts\n",
    "    tables = page. extract_tables()\n",
    "    # Iterate over each table\n",
    "    for table in tables:\n",
    "        # Get table rows as list of lists\n",
    "        rows = table.extract_table()\n",
    "        # Write rows to CSV file\n",
    "        for row in rows:\n",
    "            csv_writer.writerow(row)\n",
    "\n",
    "# Open CSV file for writing\n",
    "with open(\"D:/billionlives/KuwaitBanks/output.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    # Extract tables from each PDF page\n",
    "    for page in pdf_file:\n",
    "        extract_tables(page, csv_writer)\n",
    "\n",
    "# Close CSV file\n",
    "csv_file.close()\n",
    "\n",
    "# Close PDF file\n",
    "pdf_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "575d3100",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Page' object has no attribute 'extract_tables'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Extract tables from each PDF page\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m pdf_file:\n\u001b[1;32m---> 22\u001b[0m         \u001b[43mextract_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_writer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Close CSV file\u001b[39;00m\n\u001b[0;32m     25\u001b[0m csv_file\u001b[38;5;241m.\u001b[39mclose()\n",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36mextract_table\u001b[1;34m(page, csv_writer)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_table\u001b[39m(page, csv_writer):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Get list of page tables as dicts\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     tables \u001b[38;5;241m=\u001b[39m \u001b[43mpage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_tables\u001b[49m()\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Iterate over each table\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables:\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;66;03m# Write rows to CSV file\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Page' object has no attribute 'extract_tables'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import fitz\n",
    "\n",
    "# Open PDF file\n",
    "pdf_file = fitz.open(\"D:/billionlives/KuwaitBanks/NBK.pdf\")\n",
    "\n",
    "# Define function to extract tables from a PDF page\n",
    "def extract_table(page, csv_writer):\n",
    "    # Get list of page tables as dicts\n",
    "    tables = page.extract_tables()\n",
    "    # Iterate over each table\n",
    "    for table in tables:\n",
    "        # Write rows to CSV file\n",
    "        for row in table:\n",
    "            csv_writer.writerow(row)\n",
    "\n",
    "# Open CSV file for writing\n",
    "with open(\"D:/billionlives/KuwaitBanks/output.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    # Extract tables from each PDF page\n",
    "    for page in pdf_file:\n",
    "        extract_table(page, csv_writer)\n",
    "\n",
    "# Close CSV file\n",
    "csv_file.close()\n",
    "\n",
    "# Close PDF file\n",
    "pdf_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dfe71ad3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Page' object has no attribute 'extract_table'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Extract tables from each PDF page\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m pdf_file:\n\u001b[1;32m---> 20\u001b[0m         \u001b[43mextract_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_writer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Close CSV file\u001b[39;00m\n\u001b[0;32m     23\u001b[0m csv_file\u001b[38;5;241m.\u001b[39mclose()\n",
      "Input \u001b[1;32mIn [39]\u001b[0m, in \u001b[0;36mextract_table\u001b[1;34m(page, csv_writer)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_table\u001b[39m(page, csv_writer):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Get table data as a list of lists\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     table \u001b[38;5;241m=\u001b[39m \u001b[43mpage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_table\u001b[49m()\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Write rows to CSV file\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m table:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Page' object has no attribute 'extract_table'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import fitz\n",
    "\n",
    "# Open PDF file\n",
    "pdf_file = fitz.open(\"D:/billionlives/KuwaitBanks/NBK.pdf\")\n",
    "\n",
    "# Define function to extract tables from a PDF page\n",
    "def extract_table(page, csv_writer):\n",
    "    # Get table data as a list of lists\n",
    "    table = page.extract_table()\n",
    "    # Write rows to CSV file\n",
    "    for row in table:\n",
    "        csv_writer.writerow(row)\n",
    "\n",
    "# Open CSV file for writing\n",
    "with open(\"D:/billionlives/KuwaitBanks/output.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    # Extract tables from each PDF page\n",
    "    for page in pdf_file:\n",
    "        extract_table(page, csv_writer)\n",
    "\n",
    "# Close CSV file\n",
    "csv_file.close()\n",
    "\n",
    "# Close PDF file\n",
    "pdf_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a06991ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Page' object has no attribute 'extract_tables'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Extract tables from each PDF page\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m pdf_file:\n\u001b[1;32m---> 22\u001b[0m         \u001b[43mextract_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_writer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Close CSV file\u001b[39;00m\n\u001b[0;32m     25\u001b[0m csv_file\u001b[38;5;241m.\u001b[39mclose()\n",
      "Input \u001b[1;32mIn [40]\u001b[0m, in \u001b[0;36mextract_table\u001b[1;34m(page, csv_writer)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_table\u001b[39m(page, csv_writer):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Get list of page tables as lists of rows\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     tables \u001b[38;5;241m=\u001b[39m \u001b[43mpage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_tables\u001b[49m()\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Iterate over each table\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables:\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;66;03m# Write rows to CSV file\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Page' object has no attribute 'extract_tables'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import fitz\n",
    "\n",
    "# Open PDF file\n",
    "pdf_file = fitz.open(\"D:/billionlives/KuwaitBanks/NBK.pdf\")\n",
    "\n",
    "# Define function to extract tables from a PDF page\n",
    "def extract_table(page, csv_writer):\n",
    "    # Get list of page tables as lists of rows\n",
    "    tables = page.extract_tables()\n",
    "    # Iterate over each table\n",
    "    for table in tables:\n",
    "        # Write rows to CSV file\n",
    "        for row in table:\n",
    "            csv_writer.writerow(row)\n",
    "\n",
    "# Open CSV file for writing\n",
    "with open(\"D:/billionlives/KuwaitBanks/output.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    # Extract tables from each PDF page\n",
    "    for page in pdf_file:\n",
    "        extract_table(page, csv_writer)\n",
    "\n",
    "# Close CSV file\n",
    "csv_file.close()\n",
    "\n",
    "# Close PDF file\n",
    "pdf_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c8ddd396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import fitz\n",
    "\n",
    "# Open PDF file\n",
    "pdf_file = fitz.open(\"D:/billionlives/KuwaitBanks/BRSR/HDFC.pdf\")\n",
    "\n",
    "# Define function to extract tables from a PDF page\n",
    "def extract_table(page, csv_writer):\n",
    "    # Get all text on the page\n",
    "    text = page.get_text()\n",
    "    # Split the text into lines\n",
    "    lines = text.split(\"\\n\")\n",
    "    # Find lines that contain table data\n",
    "    table_lines = [line for line in lines if \"|\" in line]\n",
    "    # Split each table line into cells\n",
    "    tables = [line.split(\"|\") for line in table_lines]\n",
    "    # Write rows to CSV file\n",
    "    for table in tables:\n",
    "        csv_writer.writerow(table)\n",
    "\n",
    "# Open CSV file for writing\n",
    "with open(\"D:/billionlives/KuwaitBanks/BRSR/output.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    # Extract tables from each PDF page\n",
    "    for page in pdf_file:\n",
    "        extract_table(page, csv_writer)\n",
    "\n",
    "# Close CSV file\n",
    "csv_file.close()\n",
    "\n",
    "# Close PDF file\n",
    "pdf_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5919e2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import fitz\n",
    "\n",
    "# Open PDF file\n",
    "pdf_file = fitz.open(\"D:/billionlives/KuwaitBanks/BRSR/HDFC.pdf\")\n",
    "\n",
    "# Define function to extract tables from a PDF page\n",
    "def extract_table(page, csv_writer):\n",
    "    # Get all text on the page\n",
    "    text = page.get_text()\n",
    "    # Split the text into lines\n",
    "    lines = text.split(\"\\n\")\n",
    "    # Find lines that contain table data\n",
    "    table_lines = [line for line in lines if \"|\" in line]\n",
    "    # Split each table line into cells\n",
    "    tables = [line.split(\"|\") for line in table_lines]\n",
    "    # Write rows to CSV file\n",
    "    for table in tables:\n",
    "        csv_writer.writerow(table)\n",
    "        print(\"Table row:\", table)\n",
    "\n",
    "# Open CSV file for writing\n",
    "with open(\"D:/billionlives/KuwaitBanks/BRSR/output.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    # Extract tables from each PDF page\n",
    "    for page in pdf_file:\n",
    "        extract_table(page, csv_writer)\n",
    "\n",
    "# Close CSV file\n",
    "csv_file.close()\n",
    "\n",
    "# Close PDF file\n",
    "pdf_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52dbe272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyMuPDF in c:\\users\\joe\\anaconda3\\lib\\site-packages (1.20.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\joe\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\joe\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\joe\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\joe\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\joe\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\joe\\anaconda3\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e534196b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Page' object has no attribute 'get_tables'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 32>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m     doc\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[43mextract_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mD:/billionlives/KuwaitBanks/BRSR/HDFC.pdf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mD:/billionlives/KuwaitBanks/BRSR/output.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mextract_tables\u001b[1;34m(pdf_file, csv_file)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Loop through each page of the PDF file\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m doc:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Extract the tables from the current page\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     tables \u001b[38;5;241m=\u001b[39m \u001b[43mpage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tables\u001b[49m()\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# If no tables were found on the current page, skip it\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tables:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Page' object has no attribute 'get_tables'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import fitz\n",
    "\n",
    "def extract_tables(pdf_file, csv_file):\n",
    "    # Open the PDF file\n",
    "    doc = fitz.open(pdf_file)\n",
    "    \n",
    "    # Open the CSV file for writing\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        \n",
    "        # Loop through each page of the PDF file\n",
    "        for page in doc:\n",
    "            # Extract the tables from the current page\n",
    "            tables = page.get_tables()\n",
    "            \n",
    "            # If no tables were found on the current page, skip it\n",
    "            if not tables:\n",
    "                continue\n",
    "            \n",
    "            # Loop through each table on the current page\n",
    "            for table in tables:\n",
    "                # Loop through each row in the table\n",
    "                for row in table:\n",
    "                    # Write the row to the CSV file\n",
    "                    writer.writerow(row)\n",
    "    \n",
    "    # Close the PDF file\n",
    "    doc.close()\n",
    "\n",
    "# Example usage:\n",
    "extract_tables('D:/billionlives/KuwaitBanks/BRSR/HDFC.pdf', 'D:/billionlives/KuwaitBanks/BRSR/output.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aa4f86f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Page' object has no attribute 'get_tables'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m                 writer\u001b[38;5;241m.\u001b[39mwriterows(table)\n\u001b[0;32m     14\u001b[0m pdf_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:/billionlives/KuwaitBanks/BRSR/HDFC.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 15\u001b[0m \u001b[43mextract_tables_from_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mextract_tables_from_pdf\u001b[1;34m(pdf_path)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(doc\u001b[38;5;241m.\u001b[39mpage_count):\n\u001b[0;32m      7\u001b[0m     page \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mload_page(page_index)\n\u001b[1;32m----> 8\u001b[0m     table_list \u001b[38;5;241m=\u001b[39m \u001b[43mpage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tables\u001b[49m()\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m table_index, table \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(table_list):\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpage_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_table_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Page' object has no attribute 'get_tables'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_tables_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page_index in range(doc.page_count):\n",
    "        page = doc.load_page(page_index)\n",
    "        table_list = page.get_tables()\n",
    "        for table_index, table in enumerate(table_list):\n",
    "            with open(f'page_{page_index}_table_{table_index}.csv', 'w', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerows(table)\n",
    "\n",
    "pdf_path = 'D:/billionlives/KuwaitBanks/BRSR/HDFC.pdf'\n",
    "extract_tables_from_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d88b8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabula\n",
    "\n",
    "pdf_path = 'D:/billionlives/KuwaitBanks/BRSR/HDFC.pdf'\n",
    "output_path = 'D:/billionlives/KuwaitBanksCSV'\n",
    "tables = tabula.read_pdf(pdf_path, pages='all', multiple_tables=True)\n",
    "\n",
    "for i, table in enumerate(tables):\n",
    "    table.to_csv(f'table_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "924fdd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: PyMuPDF\n",
      "Version: 1.20.2\n",
      "Summary: Python bindings for the PDF toolkit and renderer MuPDF\n",
      "Home-page: https://github.com/pymupdf/PyMuPDF\n",
      "Author: Artifex\n",
      "Author-email: support@artifex.com\n",
      "License: GNU AFFERO GPL 3.0\n",
      "Location: c:\\users\\joe\\anaconda3\\lib\\site-packages\n",
      "Requires: \n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\joe\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip show PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caa57a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTSimpleVectorIndex, Document, SimpleDirectoryReader\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e7cb9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('D:/billionlives/KuwaitBanks/CSV').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "964a6564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 41692 tokens\n"
     ]
    }
   ],
   "source": [
    "index = GPTSimpleVectorIndex(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62a5a0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 167 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 7 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3,226\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"How many permanent employees are there?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c5329d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 200 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 8 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The total Scope 1 emission is 966.1 metric tonnes of CO2 equivalent in FY21.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\" What is the total Scope 1 emission ?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86e41b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 201 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 8 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The total Scope 2 emission is 2,684.1 metric tonnes of CO2 equivalent in FY21.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\" What is the total Scope 2 emission ?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf9f25f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 99 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 12 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.214 metric tonnes\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\" How much amount of plastic waste was generated in FY21?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a03723b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 90 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 8 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"How many board of directors are there?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "959b04ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 386 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 8 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"How many Key Management Personnel are there?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef791e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 93 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 11 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"How many females  are there in board of directors?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7349799a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 301 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 19 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2,463\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"How many employees including male and female  are trained on health and safety measures in FY21?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ec4a98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 535 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 10 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Employees other than Board of Directors or KMPs of the Corporation underwent various training programmes throughout the year. Owing to the pandemic, during the year, most trainings happened through blended learning which entailed virtual classroom initiatives, along with e-learning modules. Various trainings were undertaken during the year such as Prohibition of Insider Trading, Prevention of Sexual Harassment at the Workplace, Information and Cyber Security Awareness, Code of Conduct, Know Your Customer guidelines and a learning module on ESG. Other trainings included induction programmes for new recruits, leadership training, IT and cyber security and modules on soft skills, programmes on mental and physical well-being, amongst several others. The total training man days per employee was 10.47.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"Provide the details of training given to employee?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ab4ccdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 519 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 15 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All employees of the Corporation undergo various training programmes throughout the year. Owing to the pandemic, during the year, most trainings happened through blended learning which entailed virtual classroom initiatives, along with e-learning modules. Various trainings were undertaken during the year such as Prohibition of Insider Trading, Prevention of Sexual Harassment at the Workplace, Information and Cyber Security Awareness, Code of Conduct, Know Your Customer guidelines and a learning module on ESG. Other trainings included induction programmes for new recruits, leadership training, IT and cyber security and modules on soft skills, programmes on mental and physical well-being, amongst several others.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"Provide the details of training given to employee on health and safety measures?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2721531d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 92 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 11 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The details of the disclosures related to water will depend on the specific document. Generally, disclosures related to water may include information about water usage, water conservation efforts, water quality, water pollution, and water-related risks.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"Provide details of the  disclosures related to water.\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14a8b596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 99 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 13 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.291 metric tonnes\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"How much quantity of plastic waste was generated in FY20?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6f2ad3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 98 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 12 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7.016 metric tonnes\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"How much quantity of e waste was recycled in FY21?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13c76d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 127 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 15 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "It is not possible to answer this question with the given context information.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"Does this company have /offices in/around ecologically sensitive areas?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65f7167c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 93 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 11 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"How many females are there in the board of directors?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30074987",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 403 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 12 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "It is not possible to answer this question with the given context information.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"How many employees were given training on skill upgradation?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11a697b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 563 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 28 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HDFC is currently undertaking a project to provide housing for the economically weaker sections of society in India. This project is part of the Pradhan Mantri Awas Yojana (PMAY) and is being implemented in partnership with the Ministry of Housing and Urban Affairs (MoHUA). The project involves the construction of affordable housing units, as well as the rehabilitation and resettlement of existing residents. The project is being implemented in various states across India, including Maharashtra, Gujarat, Karnataka, Tamil Nadu, and Andhra Pradesh.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"Refer Provide information on project(s) for which ongoing Rehabilitation and Resettlement (R&R) is being undertaken by HDFC?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4740d922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 508 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 76 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Not Applicable\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"You are highly intelligent question answering bot that can refer Pinciple 8 of the HDFC.pdf document and answer the following question. Answer the question if it is there in the document.otherwise answer it as 'Not Applicable'.Provide information on project(s) for which ongoing Rehabilitation and Resettlement (R&R) is being undertaken by HDFC?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6690b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 963 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 75 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Not Applicable.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"You are highly intelligent question answering bot that can refer Pinciple 8 of the HDFC.pdf document and answer the following question. Answer the question if it is there in the document.otherwise answer it as 'Not Applicable'.Details of Social Impact Assessments (SIA) of projects undertaken by the entity based on applicable laws, in the current financial ?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5769babb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 526 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 70 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The HDFC.pdf document does not contain any information about the number of complaints filed by employees in FY20 about Sexual Harassment.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = index.query(\"You are highly intelligent question answering bot that can refer  the HDFC.pdf document and answer the following question based on the textual data as well as data in tables. Answer the question if it is there in the document .Answer should be a numerical value.Give the Number of complaints  filed by employees in FY20 about Sexual harassement ?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb3a254a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Not Applicable.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d5b9f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3826 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 67 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"You are highly intelligent ESG expert with 10 years of industrial experience that can refer HDFC.pdf document.Refer the data in the tables as well as textual data and answer the following question. Answer the question if it is there in the document.Answer should be a numerical value. How many females are there in board of directors?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3bc87b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
